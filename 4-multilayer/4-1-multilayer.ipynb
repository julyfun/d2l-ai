{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html\n",
    "# ReLU。获得函数可以用 torch.relu()，这不是网络层而是一个函数，用来看看的\n",
    "# sigmoid\n",
    "# tanh\n",
    "\n",
    "# 这些函数是激活函数。为什么叫激活函数呢？\n",
    "# 一个网络如果是多层全连接层，那么这还是一个线性网络，有局限性\n",
    "# 为了让他变成非线性网络，在每层全连接层输出后，我们过一个非线性的激活函数，这样就好啦\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
